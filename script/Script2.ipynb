{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Script2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP74KCbLQjkWLlocuLgXFZS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenjamimOliveira/CN_TP1/blob/main/script/Script2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn0Kuz7ESHYZ"
      },
      "source": [
        "# TP1 - CN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YSHwhWWSOgV"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P05IQQpoRLpZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "import time\n",
        "import zipfile\n",
        "import random\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cg_8tNASUZ4"
      },
      "source": [
        "## Load data sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T85hynVRSqHw",
        "outputId": "c680c895-8e59-446d-cb1e-a0d80813b501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Copy data (.zip file)\n",
        "!cp -r 'drive/MyDrive/UMinho/CN/dataset.zip' 'dataset.zip'\n",
        "\n",
        "# Extract .zip file\n",
        "with zipfile.ZipFile('dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('dataset_extr')\n",
        "\n",
        "print(\"%s minutos\" % ((time.time() - start_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "38.19596600532532 minutos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTtUPRlDTYMq"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZk3CsDPTw_9"
      },
      "source": [
        "train_dir = '/content/dataset_extr/dataset/train'\n",
        "test_dir = '/content/dataset_extr/dataset/test'\n",
        "valid_dir = '/content/dataset_extr/dataset/valid'\n",
        "\n",
        "imgDataGenerator = False\n",
        "\n",
        "if imgDataGenerator:\n",
        "  train_dtgen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "  test_dtgen = ImageDataGenerator(rescale = 1./255)\n",
        "  valid_dtgen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "  train_ds = train_dtgen.flow_from_directory(train_dir,\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32)\n",
        "  test_ds = test_dtgen.flow_from_directory(test_dir,\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32)\n",
        "  valid_ds = valid_dtgen.flow_from_directory(valid_dir,\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32)\n",
        "else:\n",
        "  train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      train_dir,\n",
        "      seed=123,\n",
        "      image_size=(64, 64),\n",
        "      batch_size=32)\n",
        "  test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      test_dir,\n",
        "      seed=123,\n",
        "      image_size=(64, 64),\n",
        "      batch_size=32)\n",
        "  valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      valid_dir,\n",
        "      seed=123,\n",
        "      image_size=(64, 64),\n",
        "      batch_size=32)\n",
        "  \n",
        "  class_names = train_ds.class_names\n",
        "\n",
        "  AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "  train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "  valid_ds = valid_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "  test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzpmpFQxUVV7"
      },
      "source": [
        "## Algoritmo GenÃ©tico"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8yPjT8_UXp0"
      },
      "source": [
        "##\n",
        "# Formato do gene [nParesConv2DMaxPooling(entre 1 e 3), learningRate, Momentum, Nesterov, batchSize, Epochs]\n",
        "#\n",
        "##\n",
        "\n",
        "learningRate = [1, 0.1, 0.01, 0.001, 0.0001]\n",
        "batchSize = [8, 128]\n",
        "epochs = [4, 20]\n",
        "num_classes = 250\n",
        "\n",
        "def random_element():\n",
        "  gene = []\n",
        "  \n",
        "  # -- nPares Conv2D/MaxPooling\n",
        "  gene.append(random.randint(1,3))\n",
        "\n",
        "  # -- Learning rate\n",
        "  gene.append(random.randint(0,4))\n",
        "\n",
        "  # -- Momentum\n",
        "  gene.append(random.randint(0, 9)/10)\n",
        "\n",
        "  # -- Nesterov\n",
        "  gene.append(random.randint(0,1))\n",
        "\n",
        "  # -- BatchSize\n",
        "  gene.append(random.randint(batchSize[0],batchSize[1]))\n",
        "\n",
        "  # -- Epochs\n",
        "  gene.append(random.randint(epochs[0], epochs[1]))\n",
        "\n",
        "  return gene\n",
        "\n",
        "def random_pool_generator(poolSize):\n",
        "  pool = []\n",
        "\n",
        "  for x in range(poolSize):\n",
        "    pool.append(random_element())\n",
        "\n",
        "  return pool\n",
        "\n",
        "def select_mating_pool(poolSize):\n",
        "  pass\n",
        "\n",
        "def crossover():\n",
        "  pass\n",
        "\n",
        "def mutation():\n",
        "  pass\n",
        "\n",
        "def model_generator(nPares):\n",
        "  mod = []\n",
        "  mod.append(layers.experimental.preprocessing.Rescaling(1./255, input_shape=(64, 64, 3)))\n",
        "  if a >= 1: \n",
        "    mod.append(layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  if a >= 2:\n",
        "    mod.append(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  if a >= 3:\n",
        "    mod.append(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  mod.append(layers.Flatten())\n",
        "  mod.append(layers.Dense(128, activation='relu'))\n",
        "  mod.append(layers.Dense(num_classes))\n",
        "  model = Sequential(mod)\n",
        "  return model\n",
        "\n",
        "\n",
        "def gene_converter(gene):\n",
        "  # -- MODELO\n",
        "  model = model_generator(gene[0])\n",
        "  # -- COMPILAR MODELO\n",
        "  opt = optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "  # Experimentar loss='sparse_categorical_crossentropy'\n",
        "  if imgDataGenerator:\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "  else:\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=loss,\n",
        "                metrics=['accuracy'])\n",
        "  # -- TREINAR MODELO\n",
        "  epochs=1\n",
        "  start_time = time.time()\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    history = model.fit(\n",
        "      train_ds,\n",
        "      validation_data=valid_ds,\n",
        "      epochs=epochs\n",
        "    )\n",
        "  print(\"%s minutos\" % ((time.time() - start_time)/60))\n",
        "  # -- AVALIAR MODELO\n",
        "  results = model.evaluate(test_ds)\n",
        "  return results\n",
        "\n",
        "poolSize = 8\n",
        "\n",
        "pool = random_pool_generator(poolSize)\n",
        "#pool[8][0] = 95\n",
        "print(pool)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7Nllrv_Uc3t"
      },
      "source": [
        "# -- guardar accuracy dos parents para evitar treinar de novo\n",
        "maximiza = []\n",
        "for gene in pool:\n",
        "  maximiza.append(gene_converter(gene=gene))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBn2sv96UdlW"
      },
      "source": [
        "for x in maximiza:\n",
        "  print(x)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}