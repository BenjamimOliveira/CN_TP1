{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Script2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNnPThLcHuSR1ZNDzWrZs5J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenjamimOliveira/CN_TP1/blob/main/script/Script2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn0Kuz7ESHYZ"
      },
      "source": [
        "# TP1 - CN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YSHwhWWSOgV"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P05IQQpoRLpZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "import time\n",
        "import zipfile\n",
        "import random\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cg_8tNASUZ4"
      },
      "source": [
        "## Load data sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T85hynVRSqHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae09a75-6f04-4b61-e479-33097104624a"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Copy data (.zip file)\n",
        "!cp -r 'drive/MyDrive/UMinho/CN/dataset.zip' 'dataset.zip'\n",
        "\n",
        "# Extract .zip file\n",
        "with zipfile.ZipFile('dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('dataset_extr')\n",
        "\n",
        "print(\"%s segundos\" % ((time.time() - start_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "33.83084487915039 minutos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTtUPRlDTYMq"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZk3CsDPTw_9",
        "outputId": "ced2851f-e1c1-408e-bfd1-d2e87d06dbae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dir = '/content/dataset_extr/dataset/train'\n",
        "test_dir = '/content/dataset_extr/dataset/test'\n",
        "valid_dir = '/content/dataset_extr/dataset/valid'\n",
        "\n",
        "imgDataGenerator = False\n",
        "\n",
        "if imgDataGenerator:\n",
        "  train_dtgen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "  test_dtgen = ImageDataGenerator(rescale = 1./255)\n",
        "  valid_dtgen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "  train_ds = train_dtgen.flow_from_directory(train_dir,\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32)\n",
        "  test_ds = test_dtgen.flow_from_directory(test_dir,\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32)\n",
        "  valid_ds = valid_dtgen.flow_from_directory(valid_dir,\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32)\n",
        "else:\n",
        "  train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      train_dir,\n",
        "      seed=123,\n",
        "      image_size=(64, 64),\n",
        "      batch_size=32)\n",
        "  test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      test_dir,\n",
        "      seed=123,\n",
        "      image_size=(64, 64),\n",
        "      batch_size=32)\n",
        "  valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      valid_dir,\n",
        "      seed=123,\n",
        "      image_size=(64, 64),\n",
        "      batch_size=32)\n",
        "  \n",
        "  class_names = train_ds.class_names\n",
        "\n",
        "  AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "  train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "  valid_ds = valid_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "  test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 35215 files belonging to 250 classes.\n",
            "Found 1250 files belonging to 250 classes.\n",
            "Found 1250 files belonging to 250 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzpmpFQxUVV7"
      },
      "source": [
        "## Algoritmo Genético"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFkF80Fdgy9K"
      },
      "source": [
        "##\n",
        "# Formato do gene [nParesConv2DMaxPooling(entre 1 e 3), learningRate, Momentum, Nesterov, batchSize, Epochs, Accuracy]\n",
        "#\n",
        "##\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ1Gd711grrS"
      },
      "source": [
        "### Gene Aleatório"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkjyX9X3gulK"
      },
      "source": [
        "def random_element():\n",
        "  gene = []\n",
        "  \n",
        "  # -- nPares Conv2D/MaxPooling\n",
        "  gene.append(random.randint(1,3))\n",
        "\n",
        "  # -- Learning rate\n",
        "  gene.append(random.randint(0,4))\n",
        "\n",
        "  # -- Momentum\n",
        "  gene.append(random.randint(0, 9)/10)\n",
        "\n",
        "  # -- Nesterov\n",
        "  gene.append(random.randint(0,1))\n",
        "\n",
        "  # -- BatchSize\n",
        "  gene.append(random.randint(batchSize[0],batchSize[1]))\n",
        "\n",
        "  # -- Epochs\n",
        "  gene.append(random.randint(epochs[0], epochs[1]))\n",
        "\n",
        "  # -- Accuracy\n",
        "  gene.append(-1)\n",
        "\n",
        "  return gene"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hpcoruUg6zn"
      },
      "source": [
        "### População Aleatória/Inicial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txvcfe15g-u0"
      },
      "source": [
        "def random_pool_generator(poolSize):\n",
        "  pool = []\n",
        "\n",
        "  for x in range(poolSize):\n",
        "    pool.append(random_element())\n",
        "\n",
        "  return pool"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvlGZrnphBrm"
      },
      "source": [
        "### Selecionar \"pais\" da próxima geração"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN12bGpXhFhT"
      },
      "source": [
        "def select_mating_pool(poolSize, pool):\n",
        "  top = round(poolSize/2)\n",
        "  def orderBy(a):\n",
        "    return a[6]\n",
        "\n",
        "  pool.sort(reverse=True, key=orderBy)\n",
        "  return pool[0:top]\n",
        "  "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1HX-Bb1_EW3"
      },
      "source": [
        "### Crossover entre 2 elementos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InK5Uwys_KM5"
      },
      "source": [
        "def mix2elements(elemento1, elemento2):\n",
        "  # tamanho do gene\n",
        "  print(\"Elemento:\")\n",
        "  print(type(elemento1))\n",
        "  print(elemento1)\n",
        "  size = len(elemento1) - 1\n",
        "  # elementos que irão ser cruzados\n",
        "  crossElements = []\n",
        "  for x in range(3):\n",
        "    rand = random.randint(0, size-1)\n",
        "    while (rand in crossElements):\n",
        "      rand = random.randint(0, size-1)\n",
        "    crossElements.append(rand)\n",
        "\n",
        "  elementoFilho = []\n",
        "  aux = 0\n",
        "\n",
        "  while aux < size:\n",
        "    if aux in crossElements:\n",
        "      elementoFilho.append(elemento1[aux])\n",
        "    else:\n",
        "      elementoFilho.append(elemento2[aux])\n",
        "    aux += 1\n",
        "  # indica que ainda não foi treinado  \n",
        "  elementoFilho.append(-1)\n",
        "  return elementoFilho"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxlQJNA6hH3p"
      },
      "source": [
        "### Selecionar que elementos vão fazer crossover"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W37OsbtKhLu8"
      },
      "source": [
        "def crossover(poolSize, pool):\n",
        "  currentPool = poolSize\n",
        "\n",
        "  # mistura o primeiro com o último\n",
        "  pool.append(mix2elements(pool[0], pool[-1]))\n",
        "\n",
        "  aux = 0\n",
        "  for i in range(int(round(poolSize/2))-1):\n",
        "    pool.append(mix2elements(pool[aux], pool[aux+1]))\n",
        "  return pool"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MnXksY_hNXy"
      },
      "source": [
        "### Mutação de uma geração"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK9D9dX8hQGr"
      },
      "source": [
        "def mutation(pool, chance):\n",
        "  chance = chance * 100\n",
        "  print(chance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aviUEouShTPU"
      },
      "source": [
        "### Gerador de CNN (segundo o gene)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqth0TzmhWjz"
      },
      "source": [
        "def model_generator(nPares):\n",
        "  mod = []\n",
        "  mod.append(layers.experimental.preprocessing.Rescaling(1./255, input_shape=(64, 64, 3)))\n",
        "  if nPares >= 1: \n",
        "    mod.append(layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  if nPares >= 2:\n",
        "    mod.append(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  if nPares >= 3:\n",
        "    mod.append(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  mod.append(layers.Flatten())\n",
        "  mod.append(layers.Dense(128, activation='relu'))\n",
        "  mod.append(layers.Dense(num_classes))\n",
        "  model = Sequential(mod)\n",
        "  return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQt_BcuHhaHx"
      },
      "source": [
        "### Conversor de gene em acurácia (treino da cnn com o gene)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_2Wq59qhfeD"
      },
      "source": [
        "def gene_converter(gene):\n",
        "  # -- MODELO\n",
        "  model = model_generator(gene[0])\n",
        "  # -- COMPILAR MODELO\n",
        "  opt = optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "  # Experimentar loss='sparse_categorical_crossentropy'\n",
        "  if imgDataGenerator:\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "  else:\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=loss,\n",
        "                metrics=['accuracy'])\n",
        "  # -- TREINAR MODELO\n",
        "  epochs=1\n",
        "  start_time = time.time()\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    history = model.fit(\n",
        "      train_ds,\n",
        "      validation_data=valid_ds,\n",
        "      epochs=epochs\n",
        "    )\n",
        "  # -- AVALIAR MODELO\n",
        "  results = model.evaluate(test_ds)\n",
        "  return results[1]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHPL7s0JhvQ4"
      },
      "source": [
        "### Algoritmo Final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7Nllrv_Uc3t",
        "outputId": "3f02e424-7c84-4b1b-927e-a286527fdf3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# --------- Parametros chave ---------\n",
        "learningRate = [1, 0.1, 0.01, 0.001, 0.0001]\n",
        "batchSize = [8, 128]\n",
        "epochs = [4, 20]\n",
        "num_classes = 250\n",
        "poolSize = 4\n",
        "mutationChance = 0.2\n",
        "# ------------------------------------\n",
        "\n",
        "pool = random_pool_generator(poolSize)\n",
        "print(pool)\n",
        "\n",
        "maximiza = []\n",
        "\n",
        "# treina a geração\n",
        "for gene in pool:\n",
        "  # -- se ainda não tiver sido treinado numa geração passada então treina\n",
        "  if gene[6] == -1:\n",
        "    gene[6] = gene_converter(gene=gene)\n",
        "  \n",
        "# seleciona metade do poolsize para se reproduzir\n",
        "pool = select_mating_pool(poolSize, pool)\n",
        "pool = crossover(poolSize, pool)\n",
        "pool = mutation(pool, mutationChance)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 2, 0.9, 1, 38, 5, -1], [3, 1, 0.9, 1, 99, 11, -1], [3, 3, 0.4, 0, 114, 4, -1], [1, 3, 0.0, 1, 109, 18, -1]]\n",
            "1101/1101 [==============================] - 5s 4ms/step - loss: 5.1493 - accuracy: 0.0365 - val_loss: 3.2161 - val_accuracy: 0.2560\n",
            "0.08097312450408936 minutos\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 3.2232 - accuracy: 0.2600\n",
            "1101/1101 [==============================] - 5s 4ms/step - loss: 5.0547 - accuracy: 0.0450 - val_loss: 3.0737 - val_accuracy: 0.2832\n",
            "0.07739024957021078 minutos\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 3.0339 - accuracy: 0.3104\n",
            "1101/1101 [==============================] - 5s 4ms/step - loss: 5.1226 - accuracy: 0.0354 - val_loss: 3.2870 - val_accuracy: 0.2512\n",
            "0.07831404209136963 minutos\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 3.2624 - accuracy: 0.2672\n",
            "1101/1101 [==============================] - 4s 3ms/step - loss: 5.5323 - accuracy: 0.0068 - val_loss: 5.5235 - val_accuracy: 0.0040\n",
            "0.06558732986450196 minutos\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 5.5235 - accuracy: 0.0040\n",
            "Elemento:\n",
            "<class 'list'>\n",
            "[3, 1, 0.9, 1, 99, 11, 0.31040000915527344]\n",
            "Elemento:\n",
            "<class 'list'>\n",
            "[3, 1, 0.9, 1, 99, 11, 0.31040000915527344]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBn2sv96UdlW",
        "outputId": "231d8bb8-a8b2-4fe5-ec6b-9399f7dfc552",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#for x in pool:\n",
        "#  print(x)\n",
        "#print(len(pool[0]))\n",
        "#print(random.randint(0,5))\n",
        "#elemento1 = [1,1,1,1,1,1,-1]\n",
        "#elemento2 = [2,2,2,2,2,2,-1]\n",
        "#print(len(elemento1)-1)\n",
        "print(pool)\n",
        "pool = mutation(pool, 0.2)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 1, 0.9, 1, 99, 11, 0.31040000915527344], [3, 3, 0.4, 0, 114, 4, 0.2671999931335449], [3, 1, 0.9, 1, 114, 4, -1], [3, 3, 0.9, 0, 99, 11, -1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}