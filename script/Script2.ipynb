{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Script2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNl6h3XCcxaMogKjNjCsZLZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenjamimOliveira/CN_TP1/blob/main/script/Script2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn0Kuz7ESHYZ"
      },
      "source": [
        "# TP1 - CN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YSHwhWWSOgV"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P05IQQpoRLpZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "import time\n",
        "import zipfile\n",
        "import random\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cg_8tNASUZ4"
      },
      "source": [
        "## Load data sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T85hynVRSqHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "780bc731-238b-4813-a76f-2e960b76f87d"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Copy data (.zip file)\n",
        "!cp -r 'drive/MyDrive/UMinho/CN/dataset.zip' 'dataset.zip'\n",
        "\n",
        "# Extract .zip file\n",
        "with zipfile.ZipFile('dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('dataset_extr')\n",
        "\n",
        "print(\"%s segundos\" % ((time.time() - start_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "33.86333394050598 segundos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTtUPRlDTYMq"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZk3CsDPTw_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1edcfe38-bd33-4238-ad04-c08d17203509"
      },
      "source": [
        "train_dir = '/content/dataset_extr/dataset/train'\n",
        "test_dir = '/content/dataset_extr/dataset/test'\n",
        "valid_dir = '/content/dataset_extr/dataset/valid'\n",
        "\n",
        "imgDataGenerator = False\n",
        "\n",
        "if imgDataGenerator:\n",
        "  train_dtgen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "  test_dtgen = ImageDataGenerator(rescale = 1./255)\n",
        "  valid_dtgen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "  train_ds = train_dtgen.flow_from_directory(train_dir,\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32)\n",
        "  test_ds = test_dtgen.flow_from_directory(test_dir,\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32)\n",
        "  valid_ds = valid_dtgen.flow_from_directory(valid_dir,\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32)\n",
        "else:\n",
        "  train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      train_dir,\n",
        "      seed=123,\n",
        "      image_size=(64, 64),\n",
        "      batch_size=32)\n",
        "  test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      test_dir,\n",
        "      seed=123,\n",
        "      image_size=(64, 64),\n",
        "      batch_size=32)\n",
        "  valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      valid_dir,\n",
        "      seed=123,\n",
        "      image_size=(64, 64),\n",
        "      batch_size=32)\n",
        "  \n",
        "  class_names = train_ds.class_names\n",
        "\n",
        "  AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "  train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "  valid_ds = valid_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "  test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 35215 files belonging to 250 classes.\n",
            "Found 1250 files belonging to 250 classes.\n",
            "Found 1250 files belonging to 250 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzpmpFQxUVV7"
      },
      "source": [
        "## Algoritmo Genético"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFkF80Fdgy9K"
      },
      "source": [
        "##\n",
        "# Formato do gene [nParesConv2DMaxPooling(entre 1 e 3), learningRate, Momentum, Nesterov, batchSize, Epochs, Accuracy]\n",
        "#\n",
        "##"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ1Gd711grrS"
      },
      "source": [
        "### Gene Aleatório"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkjyX9X3gulK"
      },
      "source": [
        "def random_element():\n",
        "  gene = []\n",
        "  \n",
        "  # -- nPares Conv2D/MaxPooling\n",
        "  gene.append(random.randint(1,3))\n",
        "\n",
        "  # -- Learning rate\n",
        "  gene.append(random.randint(0,4))\n",
        "\n",
        "  # -- Momentum\n",
        "  gene.append(random.randint(0, 9)/10)\n",
        "\n",
        "  # -- Nesterov\n",
        "  gene.append(random.randint(0,1))\n",
        "\n",
        "  # -- BatchSize\n",
        "  gene.append(random.randint(batchSize[0],batchSize[1]))\n",
        "\n",
        "  # -- Epochs\n",
        "  gene.append(random.randint(epochs[0], epochs[1]))\n",
        "\n",
        "  # -- Accuracy\n",
        "  gene.append(-1)\n",
        "\n",
        "  return gene"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hpcoruUg6zn"
      },
      "source": [
        "### População Aleatória/Inicial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txvcfe15g-u0"
      },
      "source": [
        "def random_pool_generator(poolSize):\n",
        "  pool = []\n",
        "\n",
        "  for x in range(poolSize):\n",
        "    pool.append(random_element())\n",
        "\n",
        "  return pool"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvlGZrnphBrm"
      },
      "source": [
        "### Selecionar \"pais\" da próxima geração"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN12bGpXhFhT"
      },
      "source": [
        "def select_mating_pool(poolSize, pool):\n",
        "  top = round(poolSize/2)\n",
        "  def orderBy(a):\n",
        "    return a[6]\n",
        "\n",
        "  pool.sort(reverse=True, key=orderBy)\n",
        "  return pool[0:top]\n",
        "  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1HX-Bb1_EW3"
      },
      "source": [
        "### Crossover entre 2 elementos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InK5Uwys_KM5"
      },
      "source": [
        "def mix2elements(elemento1, elemento2):\n",
        "  # tamanho do gene\n",
        "  size = len(elemento1) - 1\n",
        "  # elementos que irão ser cruzados\n",
        "  crossElements = []\n",
        "  for x in range(3):\n",
        "    rand = random.randint(0, size-1)\n",
        "    while (rand in crossElements):\n",
        "      rand = random.randint(0, size-1)\n",
        "    crossElements.append(rand)\n",
        "\n",
        "  elementoFilho = []\n",
        "  aux = 0\n",
        "\n",
        "  while aux < size:\n",
        "    if aux in crossElements:\n",
        "      elementoFilho.append(elemento1[aux])\n",
        "    else:\n",
        "      elementoFilho.append(elemento2[aux])\n",
        "    aux += 1\n",
        "  # indica que ainda não foi treinado  \n",
        "  elementoFilho.append(-1)\n",
        "  return elementoFilho"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxlQJNA6hH3p"
      },
      "source": [
        "### Selecionar que elementos vão fazer crossover"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W37OsbtKhLu8"
      },
      "source": [
        "def crossover(poolSize, pool):\n",
        "  currentPool = poolSize\n",
        "\n",
        "  # mistura o primeiro com o último\n",
        "  pool.append(mix2elements(pool[0], pool[-1]))\n",
        "\n",
        "  aux = 0\n",
        "  for i in range(int(round(poolSize/2))-1):\n",
        "    pool.append(mix2elements(pool[aux], pool[aux+1]))\n",
        "  return pool"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3GB6F1ZG828"
      },
      "source": [
        "### Mutação de um elemento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-jLmSlQG_m-"
      },
      "source": [
        "def mutate(gene, numberOfMutations):\n",
        "  size = len(gene) - 1\n",
        "  mut = []\n",
        "  for x in range(numberOfMutations):\n",
        "    rand = random.randint(0, size-1)\n",
        "    while (rand in mut):\n",
        "      rand = random.randint(0, size-1)\n",
        "    mut.append(rand)\n",
        "  \n",
        "  element = random_element()\n",
        "  for x in mut:\n",
        "    gene[x] = element[x]\n",
        "  \n",
        "  return gene\n",
        "  "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MnXksY_hNXy"
      },
      "source": [
        "### Selecionar elementos que irão sofrer mutações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK9D9dX8hQGr"
      },
      "source": [
        "def mutation(pool, chance):\n",
        "  chance = int(round(chance * 100))\n",
        "  count = 0\n",
        "  for gene in pool:\n",
        "    rand = random.randint(0,100)\n",
        "    # Decide se um gene vai ou não ser mutado\n",
        "    if gene[-1] == -1:\n",
        "      if rand < chance:\n",
        "        gene = mutate(gene, random.randint(1,2))\n",
        "      \n",
        "  return pool"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aviUEouShTPU"
      },
      "source": [
        "### Gerador de CNN (segundo o gene)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqth0TzmhWjz"
      },
      "source": [
        "def model_generator(nPares):\n",
        "  mod = []\n",
        "  mod.append(layers.experimental.preprocessing.Rescaling(1./255, input_shape=(64, 64, 3)))\n",
        "  if nPares >= 1: \n",
        "    mod.append(layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  if nPares >= 2:\n",
        "    mod.append(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  if nPares >= 3:\n",
        "    mod.append(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  mod.append(layers.Flatten())\n",
        "  mod.append(layers.Dense(128, activation='relu'))\n",
        "  mod.append(layers.Dense(num_classes))\n",
        "  model = Sequential(mod)\n",
        "  return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQt_BcuHhaHx"
      },
      "source": [
        "### Conversor de gene em acurácia (treino da cnn com o gene)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_2Wq59qhfeD"
      },
      "source": [
        "def gene_converter(gene):\n",
        "  # -- MODELO\n",
        "  model = model_generator(gene[0])\n",
        "  # -- COMPILAR MODELO\n",
        "  if gene[3] == 0:\n",
        "    nest = False\n",
        "  else:\n",
        "    nest = True\n",
        "  opt = optimizers.SGD(learning_rate=learningRate(gene[1]), momentum=gene[2], nesterov=nest)\n",
        "  # Experimentar loss='sparse_categorical_crossentropy'\n",
        "  if imgDataGenerator:\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "  else:\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  model.compile(optimizer=opt,\n",
        "                loss=loss,\n",
        "                metrics=['accuracy'])\n",
        "  # -- TREINAR MODELO\n",
        "  epochs=1\n",
        "  start_time = time.time()\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    history = model.fit(\n",
        "      train_ds,\n",
        "      validation_data=valid_ds,\n",
        "      batch_size=gene[4],\n",
        "      epochs=gene[5]\n",
        "    )\n",
        "  # -- AVALIAR MODELO\n",
        "  results = model.evaluate(test_ds)\n",
        "  return results[1]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHPL7s0JhvQ4"
      },
      "source": [
        "### Algoritmo Final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7Nllrv_Uc3t"
      },
      "source": [
        "# --------- Parametros chave ---------\n",
        "def learningRate(lr): return 1/(10 ** lr)\n",
        "batchSize = [8, 64]\n",
        "epochs = [4, 15]\n",
        "num_classes = 250\n",
        "poolSize = 8\n",
        "mutationChance = 0.4\n",
        "geracoes = 2\n",
        "# ------------------------------------\n",
        "\n",
        "pool = random_pool_generator(poolSize)\n",
        "\n",
        "for x in range(geracoes):\n",
        "  print(\"----------------------------------\")\n",
        "  print(\"GERAÇÃO \", x)\n",
        "  print(\"----------------------------------\")\n",
        "  count = 1\n",
        "  # treina a geração\n",
        "  for gene in pool:\n",
        "    print(\"Geração {ger} - Gene {gen}\".format(ger=x, gen=count))\n",
        "    print(\"Gene: \", gene)\n",
        "    count += 1\n",
        "    # -- se ainda não tiver sido treinado numa geração passada então treina\n",
        "    if gene[6] == -1:\n",
        "      gene[6] = gene_converter(gene=gene)  \n",
        "  # seleciona metade do poolsize para se reproduzir\n",
        "  pool = select_mating_pool(poolSize, pool)\n",
        "  pool = crossover(poolSize, pool)\n",
        "  pool = mutation(pool, mutationChance)\n",
        "\n",
        "count = 1\n",
        "for gene in pool:\n",
        "    print(\"Geração {ger} - Gene {gen}\".format(ger=geracoes-1, gen=count))\n",
        "    print(\"Gene: \", gene)\n",
        "    count += 1\n",
        "    # -- se ainda não tiver sido treinado numa geração passada então treina\n",
        "    if gene[6] == -1:\n",
        "      gene[6] = gene_converter(gene=gene)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gaxq4dw3Z4a2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBn2sv96UdlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0acefe3-2cda-4bfe-e37d-af59e8987ea1"
      },
      "source": [
        "#for x in pool:\n",
        "#  print(x)\n",
        "#print(len(pool[0]))\n",
        "#print(random.randint(0,5))\n",
        "elemento1 = [1,1,1,1,1,1,-1]\n",
        "#elemento2 = [2,2,2,2,2,2,-1]\n",
        "#print(len(elemento1)-1)\n",
        "\n",
        "#pool = random_pool_generator(poolSize)\n",
        "#print(pool)\n",
        "#pool = mutation(pool, 0.9)\n",
        "\n",
        "#gene = mutate(elemento1, 2)\n",
        "#print(gene)\n",
        "\n",
        "#print(elemento1[-1])\n",
        "for x in pool:\n",
        "  print(x)\n",
        "print(\"Geração {ger} - Gene {gen}\".format(ger=\"a\", gen=\"b\"))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 2, 0.8, 0, 64, 7, 0.5712000131607056]\n",
            "[2, 2, 0.8, 0, 62, 7, 0.5415999889373779]\n",
            "[2, 2, 0.7, 0, 64, 7, 0.5167999863624573]\n",
            "[2, 2, 0.7, 0, 55, 6, 0.5063999891281128]\n",
            "[2, 2, 0.8, 0, 55, 6, 0.548799991607666]\n",
            "[2, 4, 0.5, 0, 64, 7, 0.004000000189989805]\n",
            "[2, 2, 0.8, 0, 64, 7, 0.5784000158309937]\n",
            "[2, 3, 0.8, 0, 62, 14, 0.3935999870300293]\n",
            "Geração a - Gene b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q9Nk50EZpv0"
      },
      "source": [
        "# Experiências"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIN2HqKaZslL",
        "outputId": "7f6735c7-3023-408a-94fa-3875626a080d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        }
      },
      "source": [
        "gene = [3, 2, 0.5, 1, 22, 15, -1]\n",
        "\n",
        "def model_generator2(nPares):\n",
        "  mod = []\n",
        "  mod.append(layers.experimental.preprocessing.Rescaling(1./255, input_shape=(64, 64, 3)))\n",
        "  if nPares >= 1: \n",
        "    mod.append(layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  if nPares >= 2:\n",
        "    mod.append(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  if nPares >= 3:\n",
        "    mod.append(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  mod.append(layers.Flatten())\n",
        "  mod.append(layers.Dense(1024, activation='relu'))\n",
        "  mod.append(layers.Dense(num_classes))\n",
        "  model = Sequential(mod)\n",
        "  return model\n",
        "\n",
        "def gene_converter2(gene):\n",
        "  # -- MODELO\n",
        "  model = model_generator2(gene[0])\n",
        "  # -- COMPILAR MODELO\n",
        "  if gene[3] == 0:\n",
        "    nest = False\n",
        "  else:\n",
        "    nest = True\n",
        "  opt = optimizers.SGD(learning_rate=learningRate(gene[1]), momentum=gene[2], nesterov=nest)\n",
        "  # Experimentar loss='sparse_categorical_crossentropy'\n",
        "  if imgDataGenerator:\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "  else:\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  model.compile(optimizer=opt,\n",
        "                loss=loss,\n",
        "                metrics=['accuracy'])\n",
        "  # -- TREINAR MODELO\n",
        "  epochs=1\n",
        "  start_time = time.time()\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    history = model.fit(\n",
        "      train_ds,\n",
        "      validation_data=valid_ds,\n",
        "      batch_size=gene[4],\n",
        "      epochs=gene[5]\n",
        "    )\n",
        "  # -- AVALIAR MODELO\n",
        "  results = model.evaluate(test_ds)\n",
        "  return results[1]\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=1):\n",
        "  conv_base = VGG16(include_top=False,\n",
        "                     weights='imagenet', \n",
        "                     input_shape=input_shape)\n",
        "    \n",
        "    # Defines how many layers to freeze during training.\n",
        "    # Layers in the convolutional base are switched from trainable to non-trainable\n",
        "    # depending on the size of the fine-tuning parameter.\n",
        "  if fine_tune > 0:\n",
        "    for layer in conv_base.layers[:-fine_tune]:\n",
        "            layer.trainable = False\n",
        "    else:\n",
        "      for layer in conv_base.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "\n",
        "    # Create a new 'top' of the model (i.e. fully-connected layers).\n",
        "    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n",
        "  top_model = conv_base.output\n",
        "  top_model = layers.Flatten(name=\"flatten\")(top_model)\n",
        "  top_model = layers.Dense(4096, activation='relu')(top_model)\n",
        "  top_model = layers.Dense(1072, activation='relu')(top_model)\n",
        "  top_model = layers.Dropout(0.2)(top_model)\n",
        "  output_layer = layers.Dense(n_classes, activation='softmax')(top_model)\n",
        "    \n",
        "    # Group the convolutional base and new fully-connected layers into a Model object.\n",
        "  model = keras.Model(inputs=conv_base.input, outputs=output_layer)\n",
        "\n",
        "    # Compiles the model for training.\n",
        "  model.compile(optimizer=optimizer, \n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "  return model\n",
        "\n",
        "#gene[6] = gene_converter2(gene=gene) \n",
        "#print(gene)\n",
        "\n",
        "b = create_model((64,64,3), 250)\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  vgg_history = b.fit(train_ds,\n",
        "                            epochs=30,\n",
        "                            batch_size=32,\n",
        "                            validation_data=valid_ds)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1101/1101 [==============================] - 14s 12ms/step - loss: 10.6504 - accuracy: 0.0340 - val_loss: 5.1461 - val_accuracy: 0.0736\n",
            "Epoch 2/30\n",
            "1101/1101 [==============================] - 14s 12ms/step - loss: 5.2052 - accuracy: 0.0722 - val_loss: 4.8350 - val_accuracy: 0.1152\n",
            "Epoch 3/30\n",
            "1101/1101 [==============================] - 14s 12ms/step - loss: 5.0189 - accuracy: 0.1019 - val_loss: 5.0225 - val_accuracy: 0.1144\n",
            "Epoch 4/30\n",
            "1101/1101 [==============================] - 14s 12ms/step - loss: 4.9898 - accuracy: 0.1121 - val_loss: 5.2535 - val_accuracy: 0.1096\n",
            "Epoch 5/30\n",
            "1101/1101 [==============================] - 14s 12ms/step - loss: 5.0237 - accuracy: 0.1171 - val_loss: 5.1366 - val_accuracy: 0.0992\n",
            "Epoch 6/30\n",
            " 517/1101 [=============>................] - ETA: 6s - loss: 5.1547 - accuracy: 0.1017"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-85bf3788193a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                             validation_data=valid_ds)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}