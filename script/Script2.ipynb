{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Script2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOBJn2L3T3dKEIa0aiK35Jd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenjamimOliveira/CN_TP1/blob/main/script/Script2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn0Kuz7ESHYZ"
      },
      "source": [
        "# TP1 - CN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YSHwhWWSOgV"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P05IQQpoRLpZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "import time\n",
        "import zipfile\n",
        "import random\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cg_8tNASUZ4"
      },
      "source": [
        "## Load data sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T85hynVRSqHw"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Copy data (.zip file)\n",
        "!cp -r 'drive/MyDrive/UMinho/CN/dataset.zip' 'dataset.zip'\n",
        "\n",
        "# Extract .zip file\n",
        "with zipfile.ZipFile('dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('dataset_extr')\n",
        "\n",
        "print(\"%s segundos\" % ((time.time() - start_time)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTtUPRlDTYMq"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZk3CsDPTw_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa3208f-053b-4a76-d329-b7c5788efea9"
      },
      "source": [
        "train_dir = '/content/dataset_extr/dataset/train'\n",
        "test_dir = '/content/dataset_extr/dataset/test'\n",
        "valid_dir = '/content/dataset_extr/dataset/valid'\n",
        "\n",
        "imgDataGenerator = False\n",
        "\n",
        "if imgDataGenerator:\n",
        "  train_dtgen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "  test_dtgen = ImageDataGenerator(rescale = 1./255)\n",
        "  valid_dtgen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "  train_ds = train_dtgen.flow_from_directory(train_dir,\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32)\n",
        "  test_ds = test_dtgen.flow_from_directory(test_dir,\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32)\n",
        "  valid_ds = valid_dtgen.flow_from_directory(valid_dir,\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32)\n",
        "else:\n",
        "  train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      train_dir,\n",
        "      seed=123,\n",
        "      image_size=(64, 64),\n",
        "      batch_size=32)\n",
        "  test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      test_dir,\n",
        "      seed=123,\n",
        "      image_size=(64, 64),\n",
        "      batch_size=32)\n",
        "  valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      valid_dir,\n",
        "      seed=123,\n",
        "      image_size=(64, 64),\n",
        "      batch_size=32)\n",
        "  \n",
        "  class_names = train_ds.class_names\n",
        "\n",
        "  AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "  train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "  valid_ds = valid_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "  test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 35215 files belonging to 250 classes.\n",
            "Found 1250 files belonging to 250 classes.\n",
            "Found 1250 files belonging to 250 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzpmpFQxUVV7"
      },
      "source": [
        "## Algoritmo Genético"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFkF80Fdgy9K"
      },
      "source": [
        "##\n",
        "# Formato do gene [nParesConv2DMaxPooling(entre 1 e 3), learningRate, Momentum, Nesterov, batchSize, Epochs, Accuracy]\n",
        "#\n",
        "##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ1Gd711grrS"
      },
      "source": [
        "### Gene Aleatório"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkjyX9X3gulK"
      },
      "source": [
        "def random_element():\n",
        "  gene = []\n",
        "  \n",
        "  # -- nPares Conv2D/MaxPooling\n",
        "  gene.append(random.randint(1,3))\n",
        "\n",
        "  # -- Learning rate\n",
        "  gene.append(random.randint(0,4))\n",
        "\n",
        "  # -- Momentum\n",
        "  gene.append(random.randint(0, 9)/10)\n",
        "\n",
        "  # -- Nesterov\n",
        "  gene.append(random.randint(0,1))\n",
        "\n",
        "  # -- BatchSize\n",
        "  gene.append(random.randint(batchSize[0],batchSize[1]))\n",
        "\n",
        "  # -- Kernel (FilterSize)\n",
        "  gene.append(random.randint(kernelType[0], kernelType[1]))\n",
        "\n",
        "  # -- Accuracy\n",
        "  gene.append(-1)\n",
        "\n",
        "  return gene"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hpcoruUg6zn"
      },
      "source": [
        "### População Aleatória/Inicial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txvcfe15g-u0"
      },
      "source": [
        "def random_pool_generator(poolSize):\n",
        "  pool = []\n",
        "\n",
        "  for x in range(poolSize):\n",
        "    pool.append(random_element())\n",
        "\n",
        "  return pool"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvlGZrnphBrm"
      },
      "source": [
        "### Selecionar \"pais\" da próxima geração"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN12bGpXhFhT"
      },
      "source": [
        "def select_mating_pool(poolSize, pool):\n",
        "  top = round(poolSize/2)\n",
        "  def orderBy(a):\n",
        "    return a[6]\n",
        "\n",
        "  pool.sort(reverse=True, key=orderBy)\n",
        "  return pool[0:top]\n",
        "  "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1HX-Bb1_EW3"
      },
      "source": [
        "### Crossover entre 2 elementos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InK5Uwys_KM5"
      },
      "source": [
        "def mix2elements(elemento1, elemento2):\n",
        "  # tamanho do gene\n",
        "  size = len(elemento1) - 1\n",
        "  # elementos que irão ser cruzados\n",
        "  crossElements = []\n",
        "  for x in range(3):\n",
        "    rand = random.randint(0, size-1)\n",
        "    while (rand in crossElements):\n",
        "      rand = random.randint(0, size-1)\n",
        "    crossElements.append(rand)\n",
        "\n",
        "  elementoFilho = []\n",
        "  aux = 0\n",
        "\n",
        "  while aux < size:\n",
        "    if aux in crossElements:\n",
        "      elementoFilho.append(elemento1[aux])\n",
        "    else:\n",
        "      elementoFilho.append(elemento2[aux])\n",
        "    aux += 1\n",
        "  # indica que ainda não foi treinado  \n",
        "  elementoFilho.append(-1)\n",
        "  return elementoFilho"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxlQJNA6hH3p"
      },
      "source": [
        "### Selecionar que elementos vão fazer crossover"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W37OsbtKhLu8"
      },
      "source": [
        "def crossover(poolSize, pool):\n",
        "  currentPool = poolSize\n",
        "\n",
        "  # mistura o primeiro com o último\n",
        "  pool.append(mix2elements(pool[0], pool[-1]))\n",
        "\n",
        "  aux = 0\n",
        "  for i in range(int(round(poolSize/2))-1):\n",
        "    pool.append(mix2elements(pool[aux], pool[aux+1]))\n",
        "  return pool"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3GB6F1ZG828"
      },
      "source": [
        "### Mutação de um elemento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-jLmSlQG_m-"
      },
      "source": [
        "def mutate(gene, numberOfMutations):\n",
        "  size = len(gene) - 1\n",
        "  mut = []\n",
        "  for x in range(numberOfMutations):\n",
        "    rand = random.randint(0, size-1)\n",
        "    while (rand in mut):\n",
        "      rand = random.randint(0, size-1)\n",
        "    mut.append(rand)\n",
        "  \n",
        "  element = random_element()\n",
        "  for x in mut:\n",
        "    gene[x] = element[x]\n",
        "  \n",
        "  return gene\n",
        "  "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MnXksY_hNXy"
      },
      "source": [
        "### Selecionar elementos que irão sofrer mutações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK9D9dX8hQGr"
      },
      "source": [
        "def mutation(pool, chance):\n",
        "  chance = int(round(chance * 100))\n",
        "  count = 0\n",
        "  for gene in pool:\n",
        "    rand = random.randint(0,100)\n",
        "    # Decide se um gene vai ou não ser mutado\n",
        "    if gene[-1] == -1:\n",
        "      if rand < chance:\n",
        "        gene = mutate(gene, random.randint(1,2))\n",
        "      \n",
        "  return pool"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aviUEouShTPU"
      },
      "source": [
        "### Gerador de CNN (segundo o gene)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqth0TzmhWjz"
      },
      "source": [
        "def model_generator(nPares, kernelSize):\n",
        "  if kernelSize == 0:\n",
        "    ks = [3,3,3]\n",
        "  elif kernelSize == 1:\n",
        "    ks = [1,3,5]\n",
        "  else:\n",
        "    ks = [1,3,3]\n",
        "\n",
        "  mod = []\n",
        "  mod.append(layers.experimental.preprocessing.Rescaling(1./255, input_shape=(64, 64, 3)))\n",
        "  if nPares >= 1: \n",
        "    mod.append(layers.Conv2D(filters=128, kernel_size=ks[0], padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  if nPares >= 2:\n",
        "    mod.append(layers.Conv2D(64, kernel_size=ks[1], padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  if nPares >= 3:\n",
        "    mod.append(layers.Conv2D(32, kernel_size=ks[2], padding='same', activation='relu'))\n",
        "    mod.append(layers.MaxPooling2D())\n",
        "  mod.append(layers.Flatten())\n",
        "  mod.append(layers.Dense(128, activation='relu'))\n",
        "  mod.append(layers.Dense(num_classes))\n",
        "  model = Sequential(mod)\n",
        "  return model"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQt_BcuHhaHx"
      },
      "source": [
        "### Conversor de gene em acurácia (treino da cnn com o gene)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_2Wq59qhfeD"
      },
      "source": [
        "def gene_converter(gene):\n",
        "  # -- MODELO\n",
        "  model = model_generator(gene[0], gene[5])\n",
        "  # -- COMPILAR MODELO\n",
        "  if gene[3] == 0:\n",
        "    nest = False\n",
        "  else:\n",
        "    nest = True\n",
        "  opt = optimizers.SGD(learning_rate=learningRate(gene[1]), momentum=gene[2], nesterov=nest)\n",
        "  # Experimentar loss='sparse_categorical_crossentropy'\n",
        "  if imgDataGenerator:\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "  else:\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  \n",
        "  es = EarlyStopping(monitor='val_accuracy', patience=3)\n",
        "  model.compile(optimizer=opt,\n",
        "                loss=loss,\n",
        "                metrics=['accuracy'])\n",
        "  # -- TREINAR MODELO\n",
        "  epochs=1\n",
        "  start_time = time.time()\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    history = model.fit(\n",
        "      train_ds,\n",
        "      validation_data=valid_ds,\n",
        "      batch_size=gene[4],\n",
        "      callbacks=[es],\n",
        "      epochs=epochs\n",
        "    )\n",
        "  # -- AVALIAR MODELO\n",
        "  results = model.evaluate(test_ds)\n",
        "  return results[1]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHPL7s0JhvQ4"
      },
      "source": [
        "### Algoritmo Final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7Nllrv_Uc3t"
      },
      "source": [
        "# --------- Parametros chave ---------\n",
        "def learningRate(lr): return 1/(10 ** lr)\n",
        "batchSize = [8, 64]\n",
        "epochs = 15\n",
        "kernelType = [0, 2]\n",
        "num_classes = 250\n",
        "poolSize = 8\n",
        "mutationChance = 0.4\n",
        "geracoes = 2\n",
        "# ------------------------------------\n",
        "\n",
        "pool = random_pool_generator(poolSize)\n",
        "\n",
        "for x in range(geracoes):\n",
        "  print(\"----------------------------------\")\n",
        "  print(\"GERAÇÃO \", x)\n",
        "  print(\"----------------------------------\")\n",
        "  count = 1\n",
        "  # treina a geração\n",
        "  for gene in pool:\n",
        "    print(\"Geração {ger} - Gene {gen}\".format(ger=x, gen=count))\n",
        "    print(\"Gene: \", gene)\n",
        "    count += 1\n",
        "    # -- se ainda não tiver sido treinado numa geração passada então treina\n",
        "    if gene[6] == -1:\n",
        "      gene[6] = gene_converter(gene=gene)  \n",
        "  # seleciona metade do poolsize para se reproduzir\n",
        "  pool = select_mating_pool(poolSize, pool)\n",
        "  pool = crossover(poolSize, pool)\n",
        "  pool = mutation(pool, mutationChance)\n",
        "\n",
        "count = 1\n",
        "for gene in pool:\n",
        "    print(\"Geração {ger} - Gene {gen}\".format(ger=geracoes-1, gen=count))\n",
        "    print(\"Gene: \", gene)\n",
        "    count += 1\n",
        "    # -- se ainda não tiver sido treinado numa geração passada então treina\n",
        "    if gene[6] == -1:\n",
        "      gene[6] = gene_converter(gene=gene)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gaxq4dw3Z4a2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBn2sv96UdlW"
      },
      "source": [
        "#for x in pool:\n",
        "#  print(x)\n",
        "#print(len(pool[0]))\n",
        "#print(random.randint(0,5))\n",
        "#elemento1 = [1,1,1,1,1,1,-1]\n",
        "elemento2 = [2,2,0.9,2,2,2,-1]\n",
        "#print(len(elemento1)-1)\n",
        "teste = gene_converter(elemento2)\n",
        "print(teste)\n",
        "#pool = random_pool_generator(poolSize)\n",
        "#print(pool)\n",
        "#pool = mutation(pool, 0.9)\n",
        "\n",
        "#gene = mutate(elemento1, 2)\n",
        "#print(gene)\n",
        "\n",
        "#print(elemento1[-1])\n",
        "#for x in pool:\n",
        "#  print(x)\n",
        "#print(\"Geração {ger} - Gene {gen}\".format(ger=\"a\", gen=\"b\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q9Nk50EZpv0"
      },
      "source": [
        "# Experiências"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIN2HqKaZslL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c28bf769-20ae-43f4-913c-06b8a7e63c9c"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "def getVGG16Model(lastFourTrainable=False):\n",
        "  vgg_model = VGG16(weights='imagenet', input_shape=(224,224,3), include_top=True)\n",
        "\n",
        "  # Make all layers untrainable\n",
        "  for layer in vgg_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # Add fully connected layer which have 1024 neuron to VGG-16 model\n",
        "  output = vgg_model.output\n",
        "  output = layers.Flatten(name='new_flatten')(output)\n",
        "  output = layers.Dense(units=1024, activation='relu', name='new_fc')(output)\n",
        "  output = layers.Dense(units=250, activation='softmax')(output)\n",
        "  vgg_model = keras.Model(vgg_model.input, output)\n",
        "\n",
        "\n",
        "  # Compile VGG-16 model\n",
        "  vgg_model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "  vgg_model.summary()\n",
        "\n",
        "  return vgg_model\n",
        "\n",
        "def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=1):\n",
        "  conv_base = VGG16(include_top=False,\n",
        "                     weights='imagenet', \n",
        "                     input_shape=input_shape)\n",
        "    \n",
        "    # Defines how many layers to freeze during training.\n",
        "    # Layers in the convolutional base are switched from trainable to non-trainable\n",
        "    # depending on the size of the fine-tuning parameter.\n",
        "  if fine_tune > 0:\n",
        "    for layer in conv_base.layers[:-fine_tune]:\n",
        "            layer.trainable = False\n",
        "    else:\n",
        "      for layer in conv_base.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "\n",
        "    # Create a new 'top' of the model (i.e. fully-connected layers).\n",
        "    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n",
        "  top_model = conv_base.output\n",
        "  top_model = layers.Flatten(name=\"flatten\")(top_model)\n",
        "  top_model = layers.Dense(4096, activation='relu')(top_model)\n",
        "  top_model = layers.Dense(1072, activation='relu')(top_model)\n",
        "  top_model = layers.Dropout(0.2)(top_model)\n",
        "  output_layer = layers.Dense(n_classes, activation='softmax')(top_model)\n",
        "    \n",
        "    # Group the convolutional base and new fully-connected layers into a Model object.\n",
        "  model = keras.Model(inputs=conv_base.input, outputs=output_layer)\n",
        "\n",
        "    # Compiles the model for training.\n",
        "  model.compile(optimizer=optimizer, \n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "  return model\n",
        "\n",
        "#gene[6] = gene_converter2(gene=gene) \n",
        "#print(gene)\n",
        "\n",
        "b = getVGG16Model()\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  vgg_history = b.fit(train_ds,\n",
        "                            epochs=30,\n",
        "                            batch_size=32,\n",
        "                            validation_data=valid_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_34 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "new_flatten (Flatten)        (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "new_fc (Dense)               (None, 1024)              1025024   \n",
            "_________________________________________________________________\n",
            "dense_180 (Dense)            (None, 250)               256250    \n",
            "=================================================================\n",
            "Total params: 139,638,818\n",
            "Trainable params: 1,281,274\n",
            "Non-trainable params: 138,357,544\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "  17/1101 [..............................] - ETA: 6:19 - loss: 5.5211 - accuracy: 0.0022"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-195b1afb754a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                             validation_data=valid_ds)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}